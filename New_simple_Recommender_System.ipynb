{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddae8cca-e5a5-4177-a14f-c659127be429",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from surprise import Dataset, Reader, KNNBasic\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import SVD\n",
    "from surprise.model_selection import train_test_split\n",
    "from pdb import set_trace\n",
    "\n",
    "import time\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from surprise.accuracy import mse, rmse, mae\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "my_seed = 0\n",
    "random.seed(my_seed)\n",
    "np.random.seed(my_seed)\n",
    "\n",
    "# ===== Pandas Display & Warnings =====\n",
    "import warnings\n",
    "from pandas.errors import DtypeWarning\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore', category=DtypeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e68641-6b3b-4237-83a3-cfe855178058",
   "metadata": {},
   "source": [
    "### Part 1: Paths, and Loading Product Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2d9be66-49ab-48bb-a9a5-4eae509d8d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined shape: (1094411, 18)\n"
     ]
    }
   ],
   "source": [
    "pro_inf = pd.read_csv('..\\\\sephora_dataset\\\\product_info.csv')\n",
    "pro_inf_original = pro_inf.copy()\n",
    "rew_1 = pd.read_csv('..\\\\sephora_dataset\\\\reviews_0-250.csv')\n",
    "rew_2 = pd.read_csv('..\\\\sephora_dataset\\\\reviews_250-500.csv')\n",
    "rew_3 = pd.read_csv('..\\\\sephora_dataset\\\\reviews_500-750.csv')\n",
    "rew_4 = pd.read_csv('..\\\\sephora_dataset\\\\reviews_750-1250.csv')\n",
    "rew_5 = pd.read_csv('..\\\\sephora_dataset\\\\reviews_1250-end.csv')\n",
    "\n",
    "files = [\n",
    "    \"reviews_0-250.csv\",\n",
    "    \"reviews_250-500.csv\",\n",
    "    \"reviews_500-750.csv\",\n",
    "    \"reviews_750-1250.csv\",\n",
    "    \"reviews_1250-end.csv\"\n",
    "]\n",
    "\n",
    "df_reviews = pd.concat([rew_1, rew_2, rew_3, rew_4, rew_5], ignore_index=True, axis=0)\n",
    "# Remove any \"Unnamed\" columns\n",
    "df_reviews = df_reviews.loc[:, ~df_reviews.columns.str.startswith('Unnamed')]\n",
    "print(\"Combined shape:\", df_reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c92d3d97-2625-4ea0-b59e-8b57e57f0f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>brand_id</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>loves_count</th>\n",
       "      <th>rating</th>\n",
       "      <th>reviews</th>\n",
       "      <th>size</th>\n",
       "      <th>variation_type</th>\n",
       "      <th>variation_value</th>\n",
       "      <th>variation_desc</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>price_usd</th>\n",
       "      <th>value_price_usd</th>\n",
       "      <th>sale_price_usd</th>\n",
       "      <th>limited_edition</th>\n",
       "      <th>new</th>\n",
       "      <th>online_only</th>\n",
       "      <th>out_of_stock</th>\n",
       "      <th>sephora_exclusive</th>\n",
       "      <th>highlights</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>secondary_category</th>\n",
       "      <th>tertiary_category</th>\n",
       "      <th>child_count</th>\n",
       "      <th>child_max_price</th>\n",
       "      <th>child_min_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P473671</td>\n",
       "      <td>Fragrance Discovery Set</td>\n",
       "      <td>6342</td>\n",
       "      <td>19-69</td>\n",
       "      <td>6320</td>\n",
       "      <td>3.6364</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Capri Eau de Parfum:', 'Alcohol Denat. (SD A...</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['Unisex/ Genderless Scent', 'Warm &amp;Spicy Scen...</td>\n",
       "      <td>Fragrance</td>\n",
       "      <td>Value &amp; Gift Sets</td>\n",
       "      <td>Perfume Gift Sets</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P473668</td>\n",
       "      <td>La Habana Eau de Parfum</td>\n",
       "      <td>6342</td>\n",
       "      <td>19-69</td>\n",
       "      <td>3827</td>\n",
       "      <td>4.1538</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.4 oz/ 100 mL</td>\n",
       "      <td>Size + Concentration + Formulation</td>\n",
       "      <td>3.4 oz/ 100 mL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Alcohol Denat. (SD Alcohol 39C), Parfum (Fra...</td>\n",
       "      <td>195.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['Unisex/ Genderless Scent', 'Layerable Scent'...</td>\n",
       "      <td>Fragrance</td>\n",
       "      <td>Women</td>\n",
       "      <td>Perfume</td>\n",
       "      <td>2</td>\n",
       "      <td>85.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  product_id             product_name  brand_id brand_name  loves_count  \\\n",
       "0    P473671  Fragrance Discovery Set      6342      19-69         6320   \n",
       "1    P473668  La Habana Eau de Parfum      6342      19-69         3827   \n",
       "\n",
       "   rating  reviews            size                      variation_type  \\\n",
       "0  3.6364     11.0             NaN                                 NaN   \n",
       "1  4.1538     13.0  3.4 oz/ 100 mL  Size + Concentration + Formulation   \n",
       "\n",
       "  variation_value variation_desc  \\\n",
       "0             NaN            NaN   \n",
       "1  3.4 oz/ 100 mL            NaN   \n",
       "\n",
       "                                         ingredients  price_usd  \\\n",
       "0  ['Capri Eau de Parfum:', 'Alcohol Denat. (SD A...       35.0   \n",
       "1  ['Alcohol Denat. (SD Alcohol 39C), Parfum (Fra...      195.0   \n",
       "\n",
       "   value_price_usd  sale_price_usd  limited_edition  new  online_only  \\\n",
       "0              NaN             NaN                0    0            1   \n",
       "1              NaN             NaN                0    0            1   \n",
       "\n",
       "   out_of_stock  sephora_exclusive  \\\n",
       "0             0                  0   \n",
       "1             0                  0   \n",
       "\n",
       "                                          highlights primary_category  \\\n",
       "0  ['Unisex/ Genderless Scent', 'Warm &Spicy Scen...        Fragrance   \n",
       "1  ['Unisex/ Genderless Scent', 'Layerable Scent'...        Fragrance   \n",
       "\n",
       "  secondary_category  tertiary_category  child_count  child_max_price  \\\n",
       "0  Value & Gift Sets  Perfume Gift Sets            0              NaN   \n",
       "1              Women            Perfume            2             85.0   \n",
       "\n",
       "   child_min_price  \n",
       "0              NaN  \n",
       "1             30.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pro_inf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1747c08-f05a-422a-a204-ffce28482ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>is_recommended</th>\n",
       "      <th>helpfulness</th>\n",
       "      <th>total_feedback_count</th>\n",
       "      <th>total_neg_feedback_count</th>\n",
       "      <th>total_pos_feedback_count</th>\n",
       "      <th>submission_time</th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_title</th>\n",
       "      <th>skin_tone</th>\n",
       "      <th>eye_color</th>\n",
       "      <th>skin_type</th>\n",
       "      <th>hair_color</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>price_usd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1741593524</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-01</td>\n",
       "      <td>I use this with the Nudestix “Citrus Clean Bal...</td>\n",
       "      <td>Taught me how to double cleanse!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>brown</td>\n",
       "      <td>dry</td>\n",
       "      <td>black</td>\n",
       "      <td>P504322</td>\n",
       "      <td>Gentle Hydra-Gel Face Cleanser</td>\n",
       "      <td>NUDESTIX</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31423088263</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-03-21</td>\n",
       "      <td>I bought this lip mask after reading the revie...</td>\n",
       "      <td>Disappointed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P420652</td>\n",
       "      <td>Lip Sleeping Mask Intense Hydration with Vitam...</td>\n",
       "      <td>LANEIGE</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     author_id  rating  is_recommended  helpfulness  total_feedback_count  \\\n",
       "0   1741593524       5             1.0          1.0                     2   \n",
       "1  31423088263       1             0.0          NaN                     0   \n",
       "\n",
       "   total_neg_feedback_count  total_pos_feedback_count submission_time  \\\n",
       "0                         0                         2      2023-02-01   \n",
       "1                         0                         0      2023-03-21   \n",
       "\n",
       "                                         review_text  \\\n",
       "0  I use this with the Nudestix “Citrus Clean Bal...   \n",
       "1  I bought this lip mask after reading the revie...   \n",
       "\n",
       "                       review_title skin_tone eye_color skin_type hair_color  \\\n",
       "0  Taught me how to double cleanse!       NaN     brown       dry      black   \n",
       "1                      Disappointed       NaN       NaN       NaN        NaN   \n",
       "\n",
       "  product_id                                       product_name brand_name  \\\n",
       "0    P504322                     Gentle Hydra-Gel Face Cleanser   NUDESTIX   \n",
       "1    P420652  Lip Sleeping Mask Intense Hydration with Vitam...    LANEIGE   \n",
       "\n",
       "   price_usd  \n",
       "0       19.0  \n",
       "1       24.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7386aa-cc89-4bb7-a926-d31442821db3",
   "metadata": {},
   "source": [
    "### Get unique counts for the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab42c32a-29d1-43bb-b5d2-75fd907a4f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique product_id in df_reviews: 2351\n",
      "Unique product_id in pro_inf: 8494\n",
      "Unique author_id in df_reviews: 578653\n"
     ]
    }
   ],
   "source": [
    "# Get unique counts for the columns\n",
    "unique_stats = {\n",
    "    \"Unique product_id in df_reviews\": df_reviews[\"product_id\"].nunique(),\n",
    "    \"Unique product_id in pro_inf\": pro_inf[\"product_id\"].nunique(),\n",
    "    \"Unique author_id in df_reviews\": df_reviews[\"author_id\"].nunique()\n",
    "}\n",
    "\n",
    "for key, value in unique_stats.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3656bbd-0b34-41dd-a4a4-6bc6d905bad5",
   "metadata": {},
   "source": [
    "#### The difference in product_id is because df_reviews focuses on Skincare products that have user reviews, whereas pro_inf includes all products in the Sephora store."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24da9495-bd60-4a24-b7e6-50e94a4471c1",
   "metadata": {},
   "source": [
    "### Check whether each author rated each product only once, cause it's necessary for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76f4c3a5-7b84-42fd-9a25-aebd9a580395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10471 duplicate (author_id, product_id) entries.\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate author_id and product_id pairs\n",
    "duplicates = df_reviews.duplicated(subset=['author_id', 'product_id'], keep=False)\n",
    "\n",
    "# Show duplicates\n",
    "duplicate_rows = df_reviews[duplicates]\n",
    "\n",
    "# Count of duplicate (author_id, product_id) pairs\n",
    "num_duplicates = duplicate_rows.shape[0]\n",
    "\n",
    "if num_duplicates > 0:\n",
    "    print(f\"There are {num_duplicates} duplicate (author_id, product_id) entries.\")\n",
    "else:\n",
    "    print(\"Each author rated each product only once.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c563c4d9-1127-4ea7-b9a7-fac29b650832",
   "metadata": {},
   "source": [
    "### Check for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "445d77b7-fb8a-4e49-97b7-ce5ac792c611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in author_id: 0\n",
      "Missing values in product_id: 0\n",
      "Missing values in rating: 0\n"
     ]
    }
   ],
   "source": [
    "missing_values = df_reviews[['author_id', 'product_id', 'rating']].isnull().sum()\n",
    "\n",
    "print(f\"Missing values in author_id: {missing_values['author_id']}\")\n",
    "print(f\"Missing values in product_id: {missing_values['product_id']}\")\n",
    "print(f\"Missing values in rating: {missing_values['rating']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2faa6d7-6433-4d5d-af3d-fba1effb042c",
   "metadata": {},
   "source": [
    "### Aggregating Ratings by author_id and product_id. We'll group by author_id and product_id and compute the mean rating for each pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "806e5e78-91ec-4ff7-bf4b-8c61a1dc8d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in rating after aggregation: 0\n"
     ]
    }
   ],
   "source": [
    "# Group by author_id and product_id, and aggregate ratings by mean\n",
    "df_aggregated = df_reviews.groupby(['author_id', 'product_id'], as_index=False)['rating'].mean()\n",
    "\n",
    "# Check how many NaN values exist in the aggregated ratings\n",
    "missing_values_after_aggregation = df_aggregated['rating'].isnull().sum()\n",
    "print(f\"Missing values in rating after aggregation: {missing_values_after_aggregation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c07b7143-8cba-4e3a-8d15-9fcd2553aaad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>538863</td>\n",
       "      <td>P420652</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549704</td>\n",
       "      <td>P218700</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>557770</td>\n",
       "      <td>P232903</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>561736</td>\n",
       "      <td>P421998</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>561736</td>\n",
       "      <td>P445951</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  author_id product_id  rating\n",
       "0    538863    P420652     1.0\n",
       "1    549704    P218700     5.0\n",
       "2    557770    P232903     5.0\n",
       "3    561736    P421998     5.0\n",
       "4    561736    P445951     5.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aggregated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ff461bc-b4d1-4197-a365-b1676e1530d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each author rated each product only once.\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate author_id and product_id pairs\n",
    "duplicates = df_aggregated.duplicated(subset=['author_id', 'product_id'], keep=False)\n",
    "\n",
    "# Show duplicates\n",
    "duplicate_rows = df_aggregated[duplicates]\n",
    "\n",
    "# Count of duplicate (author_id, product_id) pairs\n",
    "num_duplicates = duplicate_rows.shape[0]\n",
    "\n",
    "if num_duplicates > 0:\n",
    "    print(f\"There are {num_duplicates} duplicate (author_id, product_id) entries.\")\n",
    "else:\n",
    "    print(\"Each author rated each product only once.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ea56740-0bf4-41ce-b9fe-8afa665c5e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into dataset\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(df_aggregated[['author_id', 'product_id', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f52089-7007-40d1-b3d0-57f21e0c8a65",
   "metadata": {},
   "source": [
    "### KNNBasic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d8b20d41-4b34-45d0-bc9f-1b0f852c15a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "source": [
    "# sample random trainset and testset\n",
    "# test set is made of 25% of the ratings.\n",
    "trainset, testset = train_test_split(data, test_size=0.25, random_state=my_seed)\n",
    "\n",
    "# Define a custom similarity function for the KNNBasic algorithm\n",
    "sim_options = {'name': 'pearson', 'user_based': False, 'min_support': 1}\n",
    "\n",
    "algo_knn = KNNBasic(sim_options=sim_options)\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo_knn.fit(trainset)\n",
    "predictions = algo_knn.test(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563a98ab-b85f-4886-ab6c-9fd891062ff2",
   "metadata": {},
   "source": [
    "#### Initially, we attempted to use a user-based collaborative filtering model (user_based=True) with the KNNBasic algorithm. However, this approach failed due to the extremely large number of unique users in the dataset (more than 450,000). The algorithm tries to compute a full user-user similarity matrix, which requires over 850 GB of memory — far exceeding the capacity of a standard system. This resulted in a MemoryError. To resolve this, we switched to item-based collaborative filtering (user_based=False), which computes a significantly smaller item-item similarity matrix. Since the number of unique items is much lower than the number of users, this approach is computationally feasible and successfully completed training and prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff73afb-ef1f-428b-8f0e-f1d4acadb6a5",
   "metadata": {},
   "source": [
    "### Compute MSE, RMSE and MAE on the test set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "284161ed-3507-4300-8a96-e8c236019378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.1959\n",
      "RMSE: 1.0936\n",
      "MAE:  0.7193\n"
     ]
    }
   ],
   "source": [
    "mse(predictions)\n",
    "rmse(predictions)\n",
    "mae(predictions);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e4adbf-becd-4b54-b95d-5da5cb8760fd",
   "metadata": {},
   "source": [
    "#### The model achieved an RMSE of 1.0936 and an MAE of 0.7193 on a 1–5 rating scale. This indicates that, on average, predicted ratings differ from actual ratings by about 1 point (RMSE) or 0.72 points (MAE). While not highly precise, these values are typical for baseline models like KNN and provide a solid starting point for further improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6fba9e-2350-4c3e-ab52-860e408977fa",
   "metadata": {},
   "source": [
    "### Categorical Evaluation. Precision, Recall and F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8270db16-e15d-4ce4-b221-f0d758a43245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_f1(predictions, threshold=3.5):\n",
    "    \"\"\"Return precision, recall, f1, and accuracy averaged across all users.\"\"\"\n",
    "    \n",
    "    # Group predictions by user, storing 'est' and 'true_r' in separate lists\n",
    "    user_data = defaultdict(lambda: {'est': [], 'true_r': []})\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_data[uid]['est'].append(est)\n",
    "        user_data[uid]['true_r'].append(true_r)\n",
    "    \n",
    "    # Lists to hold metrics for each user\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    accuracies = []\n",
    "    \n",
    "    for uid, data in user_data.items():\n",
    "        est = np.array(data['est'])\n",
    "        true_r = np.array(data['true_r'])\n",
    "        \n",
    "        # Vectorized computation of binary labels\n",
    "        y_true = (true_r >= threshold).astype(int)\n",
    "        y_pred = (est >= threshold).astype(int)\n",
    "        \n",
    "        # Compute confusion matrix components\n",
    "        sum_true = y_true.sum()\n",
    "        sum_pred = y_pred.sum()\n",
    "        TP = np.dot(y_true, y_pred)\n",
    "        FP = sum_pred - TP\n",
    "        FN = sum_true - TP\n",
    "        total = len(y_true)\n",
    "        TN = total - (TP + FP + FN)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision = TP / (TP + FP) if (TP + FP) != 0 else 0.0\n",
    "        recall = TP / (TP + FN) if (TP + FN) != 0 else 0.0\n",
    "        if (precision + recall) == 0:\n",
    "            f1 = 0.0\n",
    "        else:\n",
    "            f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        accuracy = (TP + TN) / total if total != 0 else 0.0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    # Compute averages\n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1s)\n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6ffd8edf-9d8f-44d2-88bd-84b164e38997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Precision: 0.79512939131075\n",
      "Avg. Recall: 0.8051404845918121\n",
      "Avg. F1: 0.7974329507334499\n",
      "Avg. Accuracy: 0.8305424252034144\n",
      "Time taken: 0.06 minutes\n"
     ]
    }
   ],
   "source": [
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# get average evaluation measures and print them\n",
    "avg_precision, avg_recall, avg_f1, avg_accuracy = precision_recall_f1(predictions, threshold=3.5)\n",
    "\n",
    "# End timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate time in minutes\n",
    "time_minutes = (end_time - start_time) / 60\n",
    "\n",
    "print(f'Avg. Precision: {avg_precision}')\n",
    "print(f'Avg. Recall: {avg_recall}')\n",
    "print(f'Avg. F1: {avg_f1}')\n",
    "print(f'Avg. Accuracy: {avg_accuracy}')\n",
    "print(f\"Time taken: {time_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22e8f90-2d18-4019-a0c4-27aa532e0ca1",
   "metadata": {},
   "source": [
    "### Rank-based Evaluation. Average Precision (AP), Precision@k, R-Precision and nDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0760b669-fb66-4eed-bc9b-82b2ced59640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, ndcg_score\n",
    "\n",
    "def measures_at_k(predictions, k=10, threshold=3.5):\n",
    "    \"\"\"Optimized version with vectorized operations and precomputed metrics\"\"\"\n",
    "    \n",
    "    # Precompute discount factors for NDCG\n",
    "    discounts = 1 / np.log2(np.arange(2, k + 2))  # Precompute for vectorized NDCG\n",
    "    \n",
    "    # Group data more efficiently using numpy arrays\n",
    "    user_data = defaultdict(lambda: {'est': [], 'true_r': []})\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_data[uid]['est'].append(est)\n",
    "        user_data[uid]['true_r'].append(true_r)\n",
    "    \n",
    "    # Initialize metric accumulators\n",
    "    ap_sum = 0.0\n",
    "    prec_sum = 0.0\n",
    "    ndcg_sum = 0.0\n",
    "    user_count = 0\n",
    "    \n",
    "    for uid, data in user_data.items():\n",
    "        est = np.array(data['est'])\n",
    "        true_r = np.array(data['true_r'])\n",
    "        n = len(est)\n",
    "        if n == 0:\n",
    "            continue\n",
    "        \n",
    "        # Vectorized sorting and threshold checks\n",
    "        sort_idx = np.argsort(-est)  # Descending sort\n",
    "        true_sorted = true_r[sort_idx]\n",
    "        y_true = (true_sorted >= threshold).astype(int)\n",
    "        y_pred = (est[sort_idx] >= threshold).astype(int)\n",
    "        \n",
    "        # Precision@k calculation\n",
    "        tp = np.sum(y_true[:k] & y_pred[:k])\n",
    "        prec_sum += tp / np.sum(y_pred[:k]) if np.any(y_pred[:k]) else 0.0\n",
    "        \n",
    "        # Average precision calculation\n",
    "        if np.sum(y_true) > 0:\n",
    "            ap_sum += average_precision_score(y_true, y_pred)\n",
    "        else:\n",
    "            ap_sum += 0.0\n",
    "        \n",
    "        # Vectorized NDCG calculation\n",
    "        if n > 1:\n",
    "            ideal_sorted = np.sort(true_r)[::-1]\n",
    "            dcg = np.sum((2 ** true_sorted[:k] - 1) * discounts[:min(k, n)])\n",
    "            idcg = np.sum((2 ** ideal_sorted[:k] - 1) * discounts[:min(k, n)])\n",
    "            ndcg_sum += dcg / idcg if idcg > 0 else 0.0\n",
    "        else:\n",
    "            ndcg_sum += 1.0 if (est[0] >= threshold and true_r[0] >= threshold) else 0.0\n",
    "        \n",
    "        user_count += 1\n",
    "\n",
    "    # Final averages\n",
    "    avg_ap = ap_sum / user_count if user_count else 0.0\n",
    "    avg_prec = prec_sum / user_count if user_count else 0.0\n",
    "    avg_ndcg = ndcg_sum / user_count if user_count else 0.0\n",
    "    \n",
    "    return avg_ap, avg_prec, avg_ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e00a9be-3982-4c93-8d9c-b3b6d76b3d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. AveragePrecision: 0.8145375300410189\n",
      "Avg. Precision@5: 0.7947792811920974\n",
      "Avg. nDCG@5: 0.8197646839367108\n",
      "Time taken: 1.25 minutes\n"
     ]
    }
   ],
   "source": [
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# get average evaluation measures and print them\n",
    "avg_average_precisions, avg_precisions, avg_ndcgs = measures_at_k(predictions, k=5, threshold=3.5)\n",
    "\n",
    "# End timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate time in minutes\n",
    "time_minutes = (end_time - start_time) / 60\n",
    "\n",
    "print(f'Avg. AveragePrecision: {avg_average_precisions}')\n",
    "print(f'Avg. Precision@5: {avg_precisions}')\n",
    "print(f'Avg. nDCG@5: {avg_ndcgs}')\n",
    "\n",
    "print(f\"Time taken: {time_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548b2ac8-5f1e-4f22-b40b-51473fd2d54c",
   "metadata": {},
   "source": [
    "#### The model is performing well. The average precision of 0.81 indicates that relevant items are generally ranked highly. With a precision@5 of 0.79, nearly 80% of the top-5 recommendations are relevant, which is a strong result. The nDCG@5 score of 0.82 suggests that relevant items are not only included in the top-5 but are also ranked well. Overall, these metrics show that recommender system is providing highly relevant and well-ordered suggestions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542063b8-9b3d-43c2-bf3e-10bc705fb431",
   "metadata": {},
   "source": [
    "### KNNWithMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "93cef213-be45-41a6-9deb-fca4c9c98e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "source": [
    "# Switch to item-based similarity (much smaller matrix)\n",
    "sim_options = {'name': 'pearson', 'user_based': False, 'min_support': 1}\n",
    "algo_knn_means = KNNWithMeans(sim_options=sim_options)\n",
    "algo_knn_means.fit(trainset)\n",
    "predictions = algo_knn_means.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9db81461-076b-4182-b32c-89d2110e43a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.1596\n",
      "RMSE: 1.0769\n",
      "MAE:  0.7218\n"
     ]
    }
   ],
   "source": [
    "mse(predictions)\n",
    "rmse(predictions)\n",
    "mae(predictions);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9ae705-eec2-438f-8c72-c4b3796b337d",
   "metadata": {},
   "source": [
    "#### Slight improvement of RMSE and MSE, suggests that KNNWithMeans is doing a better job than KNNBasic by correcting for user/item bias — even though it's a relatively small gain, it's a consistent improvement across the board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3b730a6-eeb4-48b3-8da1-f752d5b2da4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Precision: 0.7947115799494122\n",
      "Avg. Recall: 0.8037613639530574\n",
      "Avg. F1: 0.7965524209479043\n",
      "Avg. Accuracy: 0.8315693146646884\n"
     ]
    }
   ],
   "source": [
    "# get average evaluation measures and print them\n",
    "avg_precision, avg_recall, avg_f1, avg_accuracy = precision_recall_f1(predictions, threshold=3.5)\n",
    "print(f'Avg. Precision: {avg_precision}')\n",
    "print(f'Avg. Recall: {avg_recall}')\n",
    "print(f'Avg. F1: {avg_f1}')\n",
    "print(f'Avg. Accuracy: {avg_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b152a2b-6dc6-44fe-bfa4-761a30811204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. AveragePrecision: 0.8145375300410189\n",
      "Avg. Precision@5: 0.7947792811920974\n",
      "Avg. nDCG@5: 0.8197646839367108\n"
     ]
    }
   ],
   "source": [
    "# get average evaluation measures and print them\n",
    "avg_average_precisions, avg_precisions, avg_ndcgs = measures_at_k(predictions, k=5, threshold=3.5)\n",
    "print(f'Avg. AveragePrecision: {avg_average_precisions}')\n",
    "print(f'Avg. Precision@5: {avg_precisions}')\n",
    "print(f'Avg. nDCG@5: {avg_ndcgs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcc5925-2b11-4ec8-8b00-6c1ff83a1255",
   "metadata": {},
   "source": [
    "### Model-based Recommender Systems. SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ebb8bec2-8542-403b-940f-633324de96a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.2051\n",
      "RMSE: 1.0978\n",
      "MAE:  0.8432\n"
     ]
    }
   ],
   "source": [
    "algo = SVD(n_epochs=10, lr_all=0.005, reg_all=0.4)\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "mse(predictions)\n",
    "rmse(predictions)\n",
    "mae(predictions);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5de910-5904-42c5-8a7b-5a112bb435d6",
   "metadata": {},
   "source": [
    "#### The SVD model produced an RMSE of 1.0978 and an MAE of 0.8432, underperforming compared to both KNNBasic and KNNWithMeans. This suggests that, under the current settings, matrix factorization did not model user-item interactions as effectively as memory-based methods. Further tuning of SVD hyperparameters may improve performance. Due to lack of computing power and the size of the dataset, we decided not to experiment on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76feb65-8aea-4ff9-bdf2-ee93e60f35c5",
   "metadata": {},
   "source": [
    "### Content-based Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7ac2396b-95e8-489d-88ae-0b57fd6b2b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_inf['content'] = (\n",
    "    pro_inf['product_name'].fillna('') + ' '\n",
    "    + pro_inf['brand_name'].fillna('') + ' '\n",
    "    + pro_inf['primary_category'].fillna('')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2f863c69-e502-43f2-8097-3c14e1e727fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Content Dict \n",
    "item_content_dict = dict(\n",
    "    zip(pro_inf['product_id'], pro_inf['content'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b518857b-9c1e-493d-85fe-e29781f3ee31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>brand_id</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>loves_count</th>\n",
       "      <th>rating</th>\n",
       "      <th>reviews</th>\n",
       "      <th>size</th>\n",
       "      <th>variation_type</th>\n",
       "      <th>variation_value</th>\n",
       "      <th>variation_desc</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>price_usd</th>\n",
       "      <th>value_price_usd</th>\n",
       "      <th>sale_price_usd</th>\n",
       "      <th>limited_edition</th>\n",
       "      <th>new</th>\n",
       "      <th>online_only</th>\n",
       "      <th>out_of_stock</th>\n",
       "      <th>sephora_exclusive</th>\n",
       "      <th>highlights</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>secondary_category</th>\n",
       "      <th>tertiary_category</th>\n",
       "      <th>child_count</th>\n",
       "      <th>child_max_price</th>\n",
       "      <th>child_min_price</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P473671</td>\n",
       "      <td>Fragrance Discovery Set</td>\n",
       "      <td>6342</td>\n",
       "      <td>19-69</td>\n",
       "      <td>6320</td>\n",
       "      <td>3.6364</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Capri Eau de Parfum:', 'Alcohol Denat. (SD A...</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['Unisex/ Genderless Scent', 'Warm &amp;Spicy Scen...</td>\n",
       "      <td>Fragrance</td>\n",
       "      <td>Value &amp; Gift Sets</td>\n",
       "      <td>Perfume Gift Sets</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fragrance Discovery Set 19-69 Fragrance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P473668</td>\n",
       "      <td>La Habana Eau de Parfum</td>\n",
       "      <td>6342</td>\n",
       "      <td>19-69</td>\n",
       "      <td>3827</td>\n",
       "      <td>4.1538</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.4 oz/ 100 mL</td>\n",
       "      <td>Size + Concentration + Formulation</td>\n",
       "      <td>3.4 oz/ 100 mL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Alcohol Denat. (SD Alcohol 39C), Parfum (Fra...</td>\n",
       "      <td>195.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['Unisex/ Genderless Scent', 'Layerable Scent'...</td>\n",
       "      <td>Fragrance</td>\n",
       "      <td>Women</td>\n",
       "      <td>Perfume</td>\n",
       "      <td>2</td>\n",
       "      <td>85.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>La Habana Eau de Parfum 19-69 Fragrance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  product_id             product_name  brand_id brand_name  loves_count  \\\n",
       "0    P473671  Fragrance Discovery Set      6342      19-69         6320   \n",
       "1    P473668  La Habana Eau de Parfum      6342      19-69         3827   \n",
       "\n",
       "   rating  reviews            size                      variation_type  \\\n",
       "0  3.6364     11.0             NaN                                 NaN   \n",
       "1  4.1538     13.0  3.4 oz/ 100 mL  Size + Concentration + Formulation   \n",
       "\n",
       "  variation_value variation_desc  \\\n",
       "0             NaN            NaN   \n",
       "1  3.4 oz/ 100 mL            NaN   \n",
       "\n",
       "                                         ingredients  price_usd  \\\n",
       "0  ['Capri Eau de Parfum:', 'Alcohol Denat. (SD A...       35.0   \n",
       "1  ['Alcohol Denat. (SD Alcohol 39C), Parfum (Fra...      195.0   \n",
       "\n",
       "   value_price_usd  sale_price_usd  limited_edition  new  online_only  \\\n",
       "0              NaN             NaN                0    0            1   \n",
       "1              NaN             NaN                0    0            1   \n",
       "\n",
       "   out_of_stock  sephora_exclusive  \\\n",
       "0             0                  0   \n",
       "1             0                  0   \n",
       "\n",
       "                                          highlights primary_category  \\\n",
       "0  ['Unisex/ Genderless Scent', 'Warm &Spicy Scen...        Fragrance   \n",
       "1  ['Unisex/ Genderless Scent', 'Layerable Scent'...        Fragrance   \n",
       "\n",
       "  secondary_category  tertiary_category  child_count  child_max_price  \\\n",
       "0  Value & Gift Sets  Perfume Gift Sets            0              NaN   \n",
       "1              Women            Perfume            2             85.0   \n",
       "\n",
       "   child_min_price                                  content  \n",
       "0              NaN  Fragrance Discovery Set 19-69 Fragrance  \n",
       "1             30.0  La Habana Eau de Parfum 19-69 Fragrance  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pro_inf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3af3240c-73f0-44ef-ac39-3bb05c469452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach content to Surprise trainset\n",
    "# Map rawID → innerID and innerID → rawID\n",
    "raw2inner_i = trainset._raw2inner_id_items\n",
    "inner2raw_i = {inner: raw for raw, inner in raw2inner_i.items()}\n",
    "\n",
    "# trainset.n_items gives number of inner items\n",
    "trainset.content = [\n",
    "    item_content_dict.get(inner2raw_i[i], '')\n",
    "    for i in range(trainset.n_items)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4ba07a5c-20e0-435d-a729-12f95c6eb3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ContentKNN with Imports \n",
    "from surprise import AlgoBase, PredictionImpossible\n",
    "from gensim.utils import tokenize\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.similarities import SparseMatrixSimilarity\n",
    "import numpy as np\n",
    "\n",
    "class ContentKNN(AlgoBase):\n",
    "    def __init__(self, k=5):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, trainset):\n",
    "        super().fit(trainset)\n",
    "        # Tokenize each item’s content string\n",
    "        tokenized = [list(tokenize(doc)) for doc in trainset.content]\n",
    "        dct       = Dictionary(tokenized)\n",
    "        corpus    = [dct.doc2bow(doc) for doc in tokenized]\n",
    "        model     = TfidfModel(corpus)\n",
    "        self.index  = SparseMatrixSimilarity(model[corpus], num_features=len(dct))\n",
    "        self.corpus = corpus\n",
    "        self.dct    = dct\n",
    "        return self\n",
    "\n",
    "    def estimate(self, u, i):\n",
    "        # Fallback for unknown user\n",
    "        if not self.trainset.knows_user(u):\n",
    "            return self.trainset.global_mean\n",
    "\n",
    "        # Convert raw iid to inner or fallback\n",
    "        try:\n",
    "            i_inner = self.trainset.to_inner_iid(i)\n",
    "        except ValueError:\n",
    "            return self.trainset.global_mean\n",
    "\n",
    "        sims         = self.index[self.corpus[i_inner]]\n",
    "        user_ratings = self.trainset.ur[u]\n",
    "\n",
    "        # Build similarity dict for items the user rated\n",
    "        sim_dict = {iid: sims[iid] for iid, _ in user_ratings}\n",
    "\n",
    "        # Top-k positive similarities\n",
    "        top_k = sorted(sim_dict.items(), key=lambda x: x[1], reverse=True)[:self.k]\n",
    "        top_k = [(iid, sim) for iid, sim in top_k if sim > 0]\n",
    "\n",
    "        # Fallback if no neighbors\n",
    "        if not top_k:\n",
    "            ratings = [r for (_, r) in user_ratings]\n",
    "            return np.mean(ratings) if ratings else self.trainset.global_mean\n",
    "\n",
    "        # Weighted average\n",
    "        num = den = 0.0\n",
    "        for iid, sim in top_k:\n",
    "            rating = next(r for (inner_id, r) in user_ratings if inner_id == iid)\n",
    "            num += sim * rating\n",
    "            den += sim\n",
    "\n",
    "        est = num/den if den > 0 else self.trainset.global_mean\n",
    "        low, high = self.trainset.rating_scale\n",
    "        return min(high, max(low, est))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "038a4de9-2fb9-4803-a95e-d98f3524e57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.3229\n",
      "RMSE: 1.1502\n",
      "MAE:  0.8957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8956989363101945"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo_ct = ContentKNN(k=5)\n",
    "algo_ct.fit(trainset)\n",
    "predictions_ct = algo_ct.test(testset)\n",
    "\n",
    "\n",
    "mse(predictions_ct)\n",
    "rmse(predictions_ct)\n",
    "mae(predictions_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "07cdbc0e-4f83-4b54-a836-382edcf3a961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Prediction(uid='5501523111', iid='P474326', r_ui=4.0, est=4.2984068335048775, details={'was_impossible': False}),\n",
       " Prediction(uid='6753667067', iid='P443843', r_ui=5.0, est=4.2984068335048775, details={'was_impossible': False}),\n",
       " Prediction(uid=2437711485, iid='P421243', r_ui=4.0, est=4.2984068335048775, details={'was_impossible': False}),\n",
       " Prediction(uid='7099563866', iid='P12045', r_ui=5.0, est=4.2984068335048775, details={'was_impossible': False}),\n",
       " Prediction(uid='1785061628', iid='P440489', r_ui=5.0, est=4.2984068335048775, details={'was_impossible': False}),\n",
       " Prediction(uid='1198233191', iid='P433444', r_ui=5.0, est=4.2984068335048775, details={'was_impossible': False}),\n",
       " Prediction(uid='1367652116', iid='P479340', r_ui=5.0, est=4.2984068335048775, details={'was_impossible': False}),\n",
       " Prediction(uid='930452003', iid='P474953', r_ui=3.0, est=4.2984068335048775, details={'was_impossible': False}),\n",
       " Prediction(uid=1834850464, iid='P467750', r_ui=5.0, est=4.2984068335048775, details={'was_impossible': False}),\n",
       " Prediction(uid=25275365148, iid='P503642', r_ui=4.0, est=4.2984068335048775, details={'was_impossible': False})]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_ct[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d12554f4-e576-4bbb-913a-66aa194f0768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: 1741593524 item: P505461    r_ui = 3.00   est = 4.30   {'was_impossible': False}\n",
      "ContentKNN cold-start: user: 1741593524 item: P505461    r_ui = 3.00   est = 4.30   {'was_impossible': False}\n",
      "user: 1741593524 item: P505461    r_ui = 3.00   est = 4.30   {'was_impossible': True, 'reason': 'User and/or item is unknown.'}\n",
      "KNNBasic   cold-start: user: 1741593524 item: P505461    r_ui = 3.00   est = 4.30   {'was_impossible': True, 'reason': 'User and/or item is unknown.'}\n"
     ]
    }
   ],
   "source": [
    "# Cold-Start Demo\n",
    "uid_demo = df_reviews['author_id'].iloc[0]\n",
    "iid_demo = str(pro_inf['product_id'].iloc[-1])  # likely unseen by this user\n",
    "\n",
    "print(\"ContentKNN cold-start:\", algo_ct.predict(uid_demo, iid_demo, r_ui=3.0, verbose=True))\n",
    "print(\"KNNBasic   cold-start:\", algo_knn.predict(uid_demo, iid_demo, r_ui=3.0, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b1c5f75a-c57a-4490-b2bb-079cca8ffda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hybrid via .predict().est\n",
    "from surprise import AlgoBase\n",
    "import numpy as np\n",
    "\n",
    "class ContentKNNBasicHybrid(AlgoBase):\n",
    "    def __init__(self, k=5, sim_opts=None):\n",
    "        super().__init__()\n",
    "        self.k        = k\n",
    "        self.sim_opts = sim_opts or {\n",
    "            'name':'pearson',\n",
    "            'user_based':True,\n",
    "            'min_support':1\n",
    "        }\n",
    "\n",
    "    def fit(self, trainset):\n",
    "        super().fit(trainset)\n",
    "        # content‐based\n",
    "        self.content = ContentKNN(self.k).fit(trainset)\n",
    "        # collaborative\n",
    "        from surprise import KNNBasic\n",
    "        self.knn = KNNBasic(sim_options=self.sim_opts).fit(trainset)\n",
    "        return self\n",
    "\n",
    "    def estimate(self, u_inner, i_inner):\n",
    "        # convert to raw IDs for .predict\n",
    "        try:\n",
    "            raw_u = self.trainset.to_raw_uid(u_inner)\n",
    "            raw_i = self.trainset.to_raw_iid(i_inner)\n",
    "        except Exception:\n",
    "            # unseen user or item\n",
    "            return self.trainset.global_mean\n",
    "\n",
    "        # 1) content pred\n",
    "        try:\n",
    "            p1 = self.content.predict(raw_u, raw_i, r_ui=None, verbose=False)\n",
    "            r1 = p1.est\n",
    "        except Exception:\n",
    "            r1 = self.trainset.global_mean\n",
    "\n",
    "        # 2) knn pred\n",
    "        try:\n",
    "            p2 = self.knn.predict(raw_u, raw_i, r_ui=None, verbose=False)\n",
    "            r2 = p2.est\n",
    "        except Exception:\n",
    "            r2 = self.trainset.global_mean\n",
    "\n",
    "        # 3) average\n",
    "        return (r1 + r2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "afd09eb0-5436-4471-af0d-55311ff29ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "MSE: 1.1487\n",
      "RMSE: 1.0718\n",
      "MAE:  0.8002\n"
     ]
    }
   ],
   "source": [
    "hyb = ContentKNNBasicHybrid(k=5, sim_opts=sim_options)\n",
    "hyb.fit(trainset)\n",
    "predictions_hyb = hyb.test(testset)\n",
    "\n",
    "mse(clean_hyb)\n",
    "rmse(clean_hyb)\n",
    "mae(clean_hyb);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
