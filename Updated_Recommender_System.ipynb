{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddae8cca-e5a5-4177-a14f-c659127be429",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ─── Cell 1: Reproducibility ────────────────────────────────────────────────────\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "my_seed = 0\n",
    "random.seed(my_seed)\n",
    "np.random.seed(my_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a58c474d-9773-4cea-ac0e-470d387f08b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found review files: ['reviews_0-250.csv', 'reviews_1250-end.csv', 'reviews_750-1250.csv', 'reviews_250-500.csv', 'reviews_500-750.csv']\n",
      "Products: (8494, 27) Reviews: (1094411, 19) User extra: (503216, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>userID</th>\n",
       "      <th>rating</th>\n",
       "      <th>is_recommended</th>\n",
       "      <th>helpfulness</th>\n",
       "      <th>total_feedback_count</th>\n",
       "      <th>total_neg_feedback_count</th>\n",
       "      <th>total_pos_feedback_count</th>\n",
       "      <th>submission_time</th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_title</th>\n",
       "      <th>skin_tone</th>\n",
       "      <th>eye_color</th>\n",
       "      <th>skin_type</th>\n",
       "      <th>hair_color</th>\n",
       "      <th>itemID</th>\n",
       "      <th>product_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>price_usd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1741593524</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-01</td>\n",
       "      <td>I use this with the Nudestix “Citrus Clean Bal...</td>\n",
       "      <td>Taught me how to double cleanse!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>brown</td>\n",
       "      <td>dry</td>\n",
       "      <td>black</td>\n",
       "      <td>P504322</td>\n",
       "      <td>Gentle Hydra-Gel Face Cleanser</td>\n",
       "      <td>NUDESTIX</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>31423088263</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-03-21</td>\n",
       "      <td>I bought this lip mask after reading the revie...</td>\n",
       "      <td>Disappointed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P420652</td>\n",
       "      <td>Lip Sleeping Mask Intense Hydration with Vitam...</td>\n",
       "      <td>LANEIGE</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5061282401</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-03-21</td>\n",
       "      <td>My review title says it all! I get so excited ...</td>\n",
       "      <td>New Favorite Routine</td>\n",
       "      <td>light</td>\n",
       "      <td>brown</td>\n",
       "      <td>dry</td>\n",
       "      <td>blonde</td>\n",
       "      <td>P420652</td>\n",
       "      <td>Lip Sleeping Mask Intense Hydration with Vitam...</td>\n",
       "      <td>LANEIGE</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6083038851</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>I’ve always loved this formula for a long time...</td>\n",
       "      <td>Can't go wrong with any of them</td>\n",
       "      <td>NaN</td>\n",
       "      <td>brown</td>\n",
       "      <td>combination</td>\n",
       "      <td>black</td>\n",
       "      <td>P420652</td>\n",
       "      <td>Lip Sleeping Mask Intense Hydration with Vitam...</td>\n",
       "      <td>LANEIGE</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>47056667835</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>If you have dry cracked lips, this is a must h...</td>\n",
       "      <td>A must have !!!</td>\n",
       "      <td>light</td>\n",
       "      <td>hazel</td>\n",
       "      <td>combination</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P420652</td>\n",
       "      <td>Lip Sleeping Mask Intense Hydration with Vitam...</td>\n",
       "      <td>LANEIGE</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       userID  rating  is_recommended  helpfulness  \\\n",
       "0           0   1741593524     5.0             1.0          1.0   \n",
       "1           1  31423088263     1.0             0.0          NaN   \n",
       "2           2   5061282401     5.0             1.0          NaN   \n",
       "3           3   6083038851     5.0             1.0          NaN   \n",
       "4           4  47056667835     5.0             1.0          NaN   \n",
       "\n",
       "   total_feedback_count  total_neg_feedback_count  total_pos_feedback_count  \\\n",
       "0                     2                         0                         2   \n",
       "1                     0                         0                         0   \n",
       "2                     0                         0                         0   \n",
       "3                     0                         0                         0   \n",
       "4                     0                         0                         0   \n",
       "\n",
       "  submission_time                                        review_text  \\\n",
       "0      2023-02-01  I use this with the Nudestix “Citrus Clean Bal...   \n",
       "1      2023-03-21  I bought this lip mask after reading the revie...   \n",
       "2      2023-03-21  My review title says it all! I get so excited ...   \n",
       "3      2023-03-20  I’ve always loved this formula for a long time...   \n",
       "4      2023-03-20  If you have dry cracked lips, this is a must h...   \n",
       "\n",
       "                       review_title skin_tone eye_color    skin_type  \\\n",
       "0  Taught me how to double cleanse!       NaN     brown          dry   \n",
       "1                      Disappointed       NaN       NaN          NaN   \n",
       "2              New Favorite Routine     light     brown          dry   \n",
       "3   Can't go wrong with any of them       NaN     brown  combination   \n",
       "4                   A must have !!!     light     hazel  combination   \n",
       "\n",
       "  hair_color   itemID                                       product_name  \\\n",
       "0      black  P504322                     Gentle Hydra-Gel Face Cleanser   \n",
       "1        NaN  P420652  Lip Sleeping Mask Intense Hydration with Vitam...   \n",
       "2     blonde  P420652  Lip Sleeping Mask Intense Hydration with Vitam...   \n",
       "3      black  P420652  Lip Sleeping Mask Intense Hydration with Vitam...   \n",
       "4        NaN  P420652  Lip Sleeping Mask Intense Hydration with Vitam...   \n",
       "\n",
       "  brand_name  price_usd  \n",
       "0   NUDESTIX       19.0  \n",
       "1    LANEIGE       24.0  \n",
       "2    LANEIGE       24.0  \n",
       "3    LANEIGE       24.0  \n",
       "4    LANEIGE       24.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ─── Cell 2: Imports & Load Data + Derive Extra User Features ─────────────────\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 2.1 Paths\n",
    "dataset_folder = \"/Users/hamidahmad/Desktop/DataSet\"\n",
    "\n",
    "# 2.2 Products\n",
    "products = pd.read_csv(\n",
    "    os.path.join(dataset_folder, 'product_info.csv'),\n",
    "    low_memory=False\n",
    ").rename(columns={'product_id':'itemID'}).dropna(subset=['itemID'])\n",
    "products['itemID'] = products['itemID'].astype(str)\n",
    "\n",
    "# 2.3 Reviews\n",
    "review_files = [f for f in os.listdir(dataset_folder) if f.startswith('reviews')]\n",
    "print(\"Found review files:\", review_files)   # sanity check\n",
    "\n",
    "reviews = pd.concat(\n",
    "    [\n",
    "      pd.read_csv(os.path.join(dataset_folder, f), low_memory=False)\n",
    "      for f in review_files\n",
    "    ],\n",
    "    ignore_index=True\n",
    ").rename(columns={\n",
    "    'author_id':   'userID',\n",
    "    'product_id':  'itemID',\n",
    "    'rating':      'rating',\n",
    "    'review_text': 'review_text'\n",
    "})\n",
    "\n",
    "# clean & cast\n",
    "reviews['userID']      = reviews['userID'].astype(str)\n",
    "reviews['itemID']      = reviews['itemID'].astype(str)\n",
    "reviews['rating']      = pd.to_numeric(reviews['rating'], errors='coerce')\n",
    "reviews = reviews.dropna(subset=['userID','itemID','rating'])\n",
    "reviews['rating']      = reviews['rating'].astype(float)\n",
    "reviews['review_text'] = reviews.get('review_text', '').fillna('').astype(str)\n",
    "\n",
    "# 2.4 Derive extra per-user features from reviews\n",
    "user_extra = (\n",
    "    reviews\n",
    "      .groupby('userID')\n",
    "      .agg(\n",
    "         recommend_ratio           = ('is_recommended',       'mean'),\n",
    "         helpfulness_avg           = ('helpfulness',          'mean'),\n",
    "         total_feedback_count      = ('total_feedback_count', 'mean'),\n",
    "         total_pos_feedback_count  = ('total_pos_feedback_count','mean'),\n",
    "         total_neg_feedback_count  = ('total_neg_feedback_count','mean'),\n",
    "         review_count              = ('itemID',               'count')\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "print(\"Products:\", products.shape, \"Reviews:\", reviews.shape, \"User extra:\", user_extra.shape)\n",
    "display(reviews.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "998ab62e-468c-478a-a062-141f40f0ed9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset, Reader, KNNBasic, SVD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17d7513d-cb70-46a7-a4af-19e681cc92f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After limiting to top users: (17236, 19)\n",
      "Trainset users/items: 200 1313\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Generated 4309 predictions\n"
     ]
    }
   ],
   "source": [
    "# ─── Cell 3: Filter to Top-N Users, Split, and Run KNNBasic ────────────────────\n",
    "import numpy as np\n",
    "from surprise import Dataset, Reader, KNNBasic\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "# How many users to keep\n",
    "MAX_USERS = 200\n",
    "\n",
    "# 1) Pick the N users with the most ratings\n",
    "user_counts = reviews['userID'].value_counts()\n",
    "top_users   = user_counts.nlargest(MAX_USERS).index\n",
    "\n",
    "# 2) Subset reviews to only those users\n",
    "reviews_sub = reviews[reviews['userID'].isin(top_users)].copy()\n",
    "print(\"After limiting to top users:\", reviews_sub.shape)\n",
    "\n",
    "# 3) Build & split Surprise dataset\n",
    "reader    = Reader(rating_scale=(1,5))\n",
    "data_sub  = Dataset.load_from_df(\n",
    "    reviews_sub[['userID','itemID','rating']],\n",
    "    reader\n",
    ")\n",
    "trainset, testset = train_test_split(\n",
    "    data_sub,\n",
    "    test_size=0.25,\n",
    "    random_state=my_seed\n",
    ")\n",
    "print(\"Trainset users/items:\", trainset.n_users, trainset.n_items)\n",
    "\n",
    "# 4) Fit user-based KNNBasic on the smaller trainset\n",
    "sim_options = {'name':'pearson','user_based':True,'min_support':1}\n",
    "algo_knn   = KNNBasic(sim_options=sim_options)\n",
    "algo_knn.fit(trainset)\n",
    "predictions_knn = algo_knn.test(testset)\n",
    "print(f\"Generated {len(predictions_knn)} predictions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "deb15c53-0ac7-4557-911b-fcbebef3ac9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Generated 4309 predictions.\n"
     ]
    }
   ],
   "source": [
    "from surprise import KNNBasic\n",
    "\n",
    "sim_options = {'name':'pearson','user_based':True,'min_support':1}\n",
    "algo_knn   = KNNBasic(sim_options=sim_options)\n",
    "\n",
    "algo_knn.fit(trainset)\n",
    "predictions_knn = algo_knn.test(testset)\n",
    "print(f\"Generated {len(predictions_knn)} predictions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "334b1fba-81fa-4495-9ff9-983b93bbb2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.6469\n",
      "KNNBasic MSE: 0.6468565868272248\n",
      "RMSE: 0.8043\n",
      "KNNBasic RMSE: 0.8042739501110456\n",
      "MAE:  0.5324\n",
      "KNNBasic MAE: 0.5323723337262427\n"
     ]
    }
   ],
   "source": [
    "# ─── Cell 5: MSE, RMSE & MAE for KNNBasic ───────────────────────────────────────\n",
    "from surprise.accuracy import mse, rmse, mae   # ensure these are imported\n",
    "\n",
    "print(\"KNNBasic MSE:\", mse(predictions_knn))\n",
    "print(\"KNNBasic RMSE:\", rmse(predictions_knn))\n",
    "print(\"KNNBasic MAE:\",  mae(predictions_knn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "525b5a3a-8727-4066-84fc-d3b21da069ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Prediction(uid='25244252137', iid='P479352', r_ui=5.0, est=4.65668755318326, details={'was_impossible': True, 'reason': 'Not enough neighbors.'}),\n",
       " Prediction(uid='27829619285', iid='P483643', r_ui=5.0, est=4.0, details={'actual_k': 1, 'was_impossible': False})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ─── Cell 6: Inspect First Predictions ─────────────────────────────────────────\n",
    "predictions_knn[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ef0d607-ccf5-44bc-85c1-a67bdf870154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Cell 7: Precision, Recall, F1, Accuracy per User ──────────────────────────\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "def precision_recall_f1(preds, threshold=4.0):\n",
    "    user_r = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in preds:\n",
    "        user_r[uid].append((est, true_r))\n",
    "    precisions, recalls, f1s, accs = [], [], [], []\n",
    "    for uid, ur in user_r.items():\n",
    "        y_true = [1 if t>=threshold else 0 for _,t in ur]\n",
    "        y_pred = [1 if e>=threshold else 0 for e,_ in ur]\n",
    "        precisions.append( precision_score(y_true, y_pred, zero_division=0) )\n",
    "        recalls.append(    recall_score   (y_true, y_pred, zero_division=0) )\n",
    "        f1s.append(        f1_score       (y_true, y_pred, zero_division=0) )\n",
    "        accs.append(       accuracy_score (y_true, y_pred) )\n",
    "    return np.mean(precisions), np.mean(recalls), np.mean(f1s), np.mean(accs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd31a9cd-773d-4ad9-9f08-5b1a0cbd6a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Precision: 0.937\n",
      "Avg. Recall:    0.937\n",
      "Avg. F1:        0.934\n",
      "Avg. Accuracy:  0.899\n"
     ]
    }
   ],
   "source": [
    "# ─── Cell 8: Print Classification Metrics ───────────────────────────────────────\n",
    "p, r, f1, acc = precision_recall_f1(predictions_knn, threshold=4.0)\n",
    "print(f\"Avg. Precision: {p:.3f}\")\n",
    "print(f\"Avg. Recall:    {r:.3f}\")\n",
    "print(f\"Avg. F1:        {f1:.3f}\")\n",
    "print(f\"Avg. Accuracy:  {acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa1ccea3-4c3f-4f08-a35c-c2f7b1a8e0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Cell 9: Precision@K & nDCG@K ───────────────────────────────────────────────\n",
    "from sklearn.metrics import average_precision_score, ndcg_score\n",
    "\n",
    "def measures_at_k(preds, k=5, threshold=4.0):\n",
    "    user_r = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in preds:\n",
    "        user_r[uid].append((est, true_r))\n",
    "    avg_precisions, precs_at_k, ndcgs = [], [], []\n",
    "    for uid, ur in user_r.items():\n",
    "        ur.sort(key=lambda x: x[0], reverse=True)\n",
    "        y_true = [1 if t>=threshold else 0 for _,t in ur]\n",
    "        y_pred = [1 if e>=threshold else 0 for e,_ in ur]\n",
    "        if sum(y_true)>0:\n",
    "            avg_precisions.append( average_precision_score(y_true, y_pred) )\n",
    "        else:\n",
    "            avg_precisions.append(0)\n",
    "        precs_at_k.append( precision_score(y_true[:k], y_pred[:k], zero_division=0) )\n",
    "        ndcgs.append( ndcg_score([y_true], [y_pred], k=k) )\n",
    "    return np.mean(avg_precisions), np.mean(precs_at_k), np.mean(ndcgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e9d8945-656d-40c7-9efc-8cbdf3596b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. AvgPrecision: 0.938\n",
      "Avg. Precision@5:  0.956\n",
      "Avg. nDCG@5:        0.938\n"
     ]
    }
   ],
   "source": [
    "# ─── Cell 10: Print @5 Metrics ─────────────────────────────────────────────────\n",
    "ap, p5, n5 = measures_at_k(predictions_knn, k=5, threshold=4.0)\n",
    "print(f\"Avg. AvgPrecision: {ap:.3f}\")\n",
    "print(f\"Avg. Precision@5:  {p5:.3f}\")\n",
    "print(f\"Avg. nDCG@5:        {n5:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9888b9c-47d4-4535-a29d-0bcb698f0bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.3454\n",
      "SVD MSE: 0.3454002194903286\n",
      "RMSE: 0.5877\n",
      "SVD RMSE: 0.5877075969309301\n",
      "MAE:  0.3668\n",
      "SVD MAE: 0.3667851541594751\n"
     ]
    }
   ],
   "source": [
    "# ─── Cell 11: SVD Baseline ─────────────────────────────────────────────────────\n",
    "algo_svd = SVD(n_epochs=10, lr_all=0.005, reg_all=0.4)\n",
    "algo_svd.fit(trainset)\n",
    "predictions_svd = algo_svd.test(testset)\n",
    "\n",
    "print(\"SVD MSE:\", mse(predictions_svd))\n",
    "print(\"SVD RMSE:\", rmse(predictions_svd))\n",
    "print(\"SVD MAE:\", mae(predictions_svd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3e0a484-d198-464b-b71d-694bcf6a4fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Cell 12: Prepare Item “Content” ────────────────────────────────────────────\n",
    "# Concatenate product_name, brand_name, primary_category\n",
    "products['content'] = (\n",
    "    products['product_name'].fillna('') + ' '\n",
    "    + products['brand_name'].fillna('') + ' '\n",
    "    + products['primary_category'].fillna('')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe012689-d27f-4b07-95c4-ddc3f2cbf790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Cell 13: Build Content Dict ───────────────────────────────────────────────\n",
    "item_content_dict = dict(\n",
    "    zip(products['itemID'], products['content'])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43f29f44-2a97-415f-8b34-34bb751a4810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Cell 14: Attach content to Surprise trainset ──────────────────────────────\n",
    "# Map rawID → innerID and innerID → rawID\n",
    "raw2inner_i = trainset._raw2inner_id_items\n",
    "inner2raw_i = {inner: raw for raw, inner in raw2inner_i.items()}\n",
    "\n",
    "# trainset.n_items gives number of inner items\n",
    "trainset.content = [\n",
    "    item_content_dict.get(inner2raw_i[i], '')\n",
    "    for i in range(trainset.n_items)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1180662f-2b1f-4515-9b00-5e15ca47b019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Cell 15: Define ContentKNN (fixed) ─────────────────────────────────────────\n",
    "from surprise import AlgoBase, PredictionImpossible\n",
    "\n",
    "class ContentKNN(AlgoBase):\n",
    "    def __init__(self, k=5):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, trainset):\n",
    "        super().fit(trainset)\n",
    "        # Build TF-IDF index as before\n",
    "        tokenized = [list(tokenize(doc)) for doc in trainset.content]\n",
    "        dct       = Dictionary(tokenized)\n",
    "        corpus    = [dct.doc2bow(doc) for doc in tokenized]\n",
    "        model     = TfidfModel(corpus)\n",
    "        self.index   = SparseMatrixSimilarity(model[corpus], num_features=len(dct))\n",
    "        self.corpus  = corpus\n",
    "        self.dct     = dct\n",
    "        return self\n",
    "\n",
    "    def estimate(self, u, i):\n",
    "        # 1) Ensure user is known\n",
    "        if not self.trainset.knows_user(u):\n",
    "            raise PredictionImpossible(\"Unknown user.\")\n",
    "\n",
    "        # 2) Convert raw itemID -> innerID\n",
    "        try:\n",
    "            i_inner = self.trainset.to_inner_iid(i)\n",
    "        except ValueError:\n",
    "            raise PredictionImpossible(\"Unknown item.\")\n",
    "\n",
    "        # 3) Compute similarities vector for this item\n",
    "        sims = self.index[self.corpus[i_inner]]\n",
    "\n",
    "        # 4) Gather all items this user has rated (inner IDs & ratings)\n",
    "        user_ratings = self.trainset.ur[u]  # list of (inner_iid, rating)\n",
    "\n",
    "        # 5) Build a dict of sim scores for only those items\n",
    "        sim_dict = {iid: sims[iid] for (iid, _) in user_ratings}\n",
    "\n",
    "        # 6) Select top-k most similar\n",
    "        top_k = sorted(sim_dict.items(), key=lambda x: x[1], reverse=True)[: self.k]\n",
    "        # Filter out non-positive similarities\n",
    "        top_k = [(iid, sim) for iid, sim in top_k if sim > 0]\n",
    "\n",
    "        if not top_k:\n",
    "            raise PredictionImpossible(\"No similar items found.\")\n",
    "\n",
    "        # 7) Compute weighted average\n",
    "        num = 0.0\n",
    "        den = 0.0\n",
    "        for iid, sim in top_k:\n",
    "            # find the true rating for this iid\n",
    "            rating = next(r for (inner_id, r) in user_ratings if inner_id == iid)\n",
    "            num += sim * rating\n",
    "            den += sim\n",
    "\n",
    "        if den == 0:\n",
    "            raise PredictionImpossible(\"All similarities are zero.\")\n",
    "\n",
    "        est = num / den\n",
    "        # 8) Clip to valid rating range\n",
    "        low, high = self.trainset.rating_scale\n",
    "        return min(high, max(low, est))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "877069d4-47d5-49a0-a3aa-39ef1383a891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.5280\n",
      "ContentKNN MSE: 0.5279998999924175\n",
      "RMSE: 0.7266\n",
      "ContentKNN RMSE: 0.7266360161679418\n",
      "MAE:  0.5159\n",
      "ContentKNN MAE: 0.5158751295701698\n"
     ]
    }
   ],
   "source": [
    "# ─── Cell 16: Train & Eval ContentKNN ──────────────────────────────────────────\n",
    "algo_ct = ContentKNN(k=5)\n",
    "algo_ct.fit(trainset)\n",
    "predictions_ct = algo_ct.test(testset)\n",
    "\n",
    "print(\"ContentKNN MSE:\", mse(predictions_ct))\n",
    "print(\"ContentKNN RMSE:\", rmse(predictions_ct))\n",
    "print(\"ContentKNN MAE:\", mae(predictions_ct))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bddda1d9-3312-44e3-bcd4-67bdda09f1ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Prediction(uid='25244252137', iid='P479352', r_ui=5.0, est=4.65668755318326, details={'was_impossible': True, 'reason': 'Unknown item.'}),\n",
       " Prediction(uid='27829619285', iid='P483643', r_ui=5.0, est=4.65668755318326, details={'was_impossible': True, 'reason': 'Unknown item.'}),\n",
       " Prediction(uid='11950294958', iid='P475951', r_ui=4.0, est=4.65668755318326, details={'was_impossible': True, 'reason': 'Unknown item.'}),\n",
       " Prediction(uid='2776710623', iid='P415667', r_ui=5.0, est=4.65668755318326, details={'was_impossible': True, 'reason': 'Unknown item.'}),\n",
       " Prediction(uid='2661325544', iid='P474832', r_ui=5.0, est=4.65668755318326, details={'was_impossible': True, 'reason': 'Unknown item.'}),\n",
       " Prediction(uid='6333148846', iid='P500288', r_ui=4.0, est=4.65668755318326, details={'was_impossible': True, 'reason': 'Unknown item.'}),\n",
       " Prediction(uid='12640060683', iid='P461935', r_ui=5.0, est=4.65668755318326, details={'was_impossible': True, 'reason': 'Unknown item.'}),\n",
       " Prediction(uid='25107944731', iid='P408230', r_ui=5.0, est=4.65668755318326, details={'was_impossible': True, 'reason': 'Unknown item.'}),\n",
       " Prediction(uid='5178256485', iid='P379707', r_ui=5.0, est=4.65668755318326, details={'was_impossible': True, 'reason': 'Unknown item.'}),\n",
       " Prediction(uid='11109420515', iid='P480447', r_ui=5.0, est=4.65668755318326, details={'was_impossible': True, 'reason': 'Unknown item.'})]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ─── Cell 17: Inspect ContentKNN Predictions ───────────────────────────────────\n",
    "predictions_ct[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5637f391-dad5-4c51-91b9-09dd84cc414a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: 1741593524 item: P505461    r_ui = 3.00   est = 4.66   {'was_impossible': True, 'reason': 'Unknown user.'}\n",
      "ContentKNN cold-start: user: 1741593524 item: P505461    r_ui = 3.00   est = 4.66   {'was_impossible': True, 'reason': 'Unknown user.'}\n",
      "user: 1741593524 item: P505461    r_ui = 3.00   est = 4.66   {'was_impossible': True, 'reason': 'User and/or item is unknown.'}\n",
      "KNNBasic   cold-start: user: 1741593524 item: P505461    r_ui = 3.00   est = 4.66   {'was_impossible': True, 'reason': 'User and/or item is unknown.'}\n"
     ]
    }
   ],
   "source": [
    "# ─── Cell 18: Cold-Start Demo ──────────────────────────────────────────────────\n",
    "uid_demo = reviews['userID'].iloc[0]\n",
    "iid_demo = str(products['itemID'].iloc[-1])  # likely unseen by this user\n",
    "\n",
    "print(\"ContentKNN cold-start:\", algo_ct.predict(uid_demo, iid_demo, r_ui=3.0, verbose=True))\n",
    "print(\"KNNBasic   cold-start:\", algo_knn.predict(uid_demo, iid_demo, r_ui=3.0, verbose=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cb55714b-8e60-4e48-939a-a4e5e7969680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Cell 19: Define Hybrid (Content + KNNBasic) ───────────────────────────────\n",
    "class ContentKNNBasicHybrid(AlgoBase):\n",
    "    def __init__(self, k=5, sim_opts=None):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.sim_opts = sim_opts or {'name':'pearson','user_based':True,'min_support':1}\n",
    "\n",
    "    def fit(self, trainset):\n",
    "        super().fit(trainset)\n",
    "        self.content = ContentKNN(self.k).fit(trainset)\n",
    "        self.knn     = KNNBasic(sim_options=self.sim_opts).fit(trainset)\n",
    "        return self\n",
    "\n",
    "    def estimate(self, u, i):\n",
    "        if not self.trainset.knows_user(u):\n",
    "            raise PredictionImpossible(\"Unknown user.\")\n",
    "        r1 = self.content.estimate(u, i)\n",
    "        r2 = self.knn.estimate(u, i)\n",
    "        return (r1 + r2) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7f409d45-08d5-4fbf-8bca-3304f6f9a05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "MSE: 0.5280\n",
      "Hybrid MSE: 0.5279998999924175\n",
      "RMSE: 0.7266\n",
      "Hybrid RMSE: 0.7266360161679418\n",
      "MAE:  0.5159\n",
      "Hybrid MAE: 0.5158751295701698\n"
     ]
    }
   ],
   "source": [
    "# ─── Cell 20: Train & Eval Hybrid ──────────────────────────────────────────────\n",
    "hyb = ContentKNNBasicHybrid(k=5, sim_opts=sim_options)\n",
    "hyb.fit(trainset)\n",
    "predictions_hyb = hyb.test(testset)\n",
    "\n",
    "print(\"Hybrid MSE:\", mse(predictions_hyb))\n",
    "print(\"Hybrid RMSE:\", rmse(predictions_hyb))\n",
    "print(\"Hybrid MAE:\", mae(predictions_hyb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "faf1d190-4e89-4146-96e6-ba8493d4aeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Cell 21: Top-N Utility ────────────────────────────────────────────────────\n",
    "def get_top_n(preds, n=10):\n",
    "    top_n = defaultdict(list)\n",
    "    for uid,iid,true,est,_ in preds:\n",
    "        top_n[uid].append((iid,est))\n",
    "    for uid,ratings in top_n.items():\n",
    "        ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = ratings[:n]\n",
    "    return top_n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b2c7aca4-64f6-4afe-b8ce-6a0f1a02e630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 for user 1741593524:\n"
     ]
    }
   ],
   "source": [
    "# ─── Cell 22: Show Top-5 for a Sample User ─────────────────────────────────────\n",
    "top_n = get_top_n(predictions_hyb, n=5)\n",
    "sample_uid = reviews['userID'].iloc[0]\n",
    "print(f\"Top-5 for user {sample_uid}:\")\n",
    "for i,(iid,score) in enumerate(top_n[sample_uid],1):\n",
    "    name = products.loc[products['itemID']==iid,'product_name'].iloc[0]\n",
    "    print(f\"{i}. {name} (score {score:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "44654cc4-f499-4e3e-9e90-daf8b55a476c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation RMSE: 1.0979544588370889\n",
      "Best params: {'n_epochs': 10, 'lr_all': 0.005, 'reg_all': 0.4}\n",
      "RMSE: 1.0334\n",
      "Biased RMSE on A: 1.0333957182336815\n",
      "RMSE: 1.0951\n",
      "Unbiased RMSE on B: 1.0951016979093584\n"
     ]
    }
   ],
   "source": [
    "# ─── Cell 23: Full-Data GridSearchCV for SVD ────────────────────────────────────\n",
    "import random\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "# Rebuild the full Dataset object\n",
    "reader = Reader(rating_scale=(1,5))\n",
    "data   = Dataset.load_from_df(reviews[['userID','itemID','rating']], reader)\n",
    "\n",
    "# 1) Shuffle & split raw_ratings into A (80%) and B (20%)\n",
    "raw_ratings = data.raw_ratings.copy()\n",
    "random.shuffle(raw_ratings)\n",
    "split = int(0.8 * len(raw_ratings))\n",
    "A_raw = raw_ratings[:split]\n",
    "B_raw = raw_ratings[split:]\n",
    "\n",
    "data.raw_ratings = A_raw  # now data contains only A\n",
    "\n",
    "# 2) GridSearchCV on A\n",
    "param_grid = {\n",
    "    'n_epochs': [5, 10],\n",
    "    'lr_all'  : [0.002, 0.005],\n",
    "    'reg_all' : [0.4, 0.6]\n",
    "}\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3, n_jobs=-1)\n",
    "gs.fit(data)\n",
    "\n",
    "print(\"Best validation RMSE:\", gs.best_score['rmse'])\n",
    "print(\"Best params:\", gs.best_params['rmse'])\n",
    "\n",
    "# 3) Retrain on full A and evaluate biased/unbiased\n",
    "best_algo = gs.best_estimator['rmse']\n",
    "full_train = data.build_full_trainset()\n",
    "best_algo.fit(full_train)\n",
    "\n",
    "from surprise.accuracy import rmse\n",
    "\n",
    "# Biased accuracy on A\n",
    "pred_A = best_algo.test(full_train.build_testset())\n",
    "print(\"Biased RMSE on A:\", rmse(pred_A))\n",
    "\n",
    "# Unbiased accuracy on B\n",
    "pred_B = best_algo.test(data.construct_testset(B_raw))\n",
    "print(\"Unbiased RMSE on B:\", rmse(pred_B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f73ca73e-90e2-475f-a583-9cc06167bc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Held-out positives for user: 1741593524\n"
     ]
    }
   ],
   "source": [
    "# ─── Cell 24: Show Held-Out Positives for sample_uid in B_raw ─────────────────\n",
    "uid = sample_uid  # as defined in Cell 22\n",
    "\n",
    "# B_raw contains tuples like (user, item, rating[, ...])\n",
    "# We only care about positions 0 (user) and 1 (item)\n",
    "relevant = [entry[1] for entry in B_raw if entry[0] == uid]\n",
    "\n",
    "print(\"Held-out positives for user:\", uid)\n",
    "for iid in relevant:\n",
    "    name = products.loc[products['itemID'] == iid, 'product_name'].iloc[0]\n",
    "    print(f\" • {iid} → {name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
